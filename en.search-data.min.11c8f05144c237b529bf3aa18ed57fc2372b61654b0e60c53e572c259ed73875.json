[{"id":0,"href":"/docs/home/","title":"Introduction","section":"Docs","content":"Hi ðŸ‘‹ #  "},{"id":1,"href":"/docs/blog/","title":"Differentiable Programming is Easy with Flux","section":"Docs","content":" MathJax = { tex: { inlineMath: [['$', '$'], ['\\\\(', '\\\\)']] }, svg: { fontCache: 'global' } };   -- Flux has taken some major strides in the past couple of years since it has been out. But its verstality can be subtle to grasp wothout actually using it. So this series is for bringing to notice how best to take advantage of Flux and its gradient-taking backend (automatic differentiation: AD for short) Zygote.\nStarting with a bit of housekeeping. This piece will introduce some basic guidelines to Julia programming and should hopefully help with your understanding of the language and using it with a few neat tricks. Another task is to clarify what Flux and its ecosystem isn\u0026rsquo;t. It isn\u0026rsquo;t a strictly deep learning library, although it does have most of the primitives for deep learning defined. It is essentially a framework for differentiable programming.\nFor a TL;DR, differentiable programming ($\\partial$P) is a way of treating arbitrary programs as differentiable. Put it easily, it is a generalisation of the way we treat deep learning as consisting of a forward pass and a backwards pass. It applies the chain rule (refer the equation below) to every operatoin that takes place in a program. The record and sequence of the operations to every element in the code is the code itself! Specifically, it is the AST of the code.\n$$ \\frac{ d{ (f \\circ g)(x) }}{ d{x}} = \\frac{\\partial f}{\\partial g} \\times \\frac{\\partial g}{\\partial x} $$\nIt replaces the standard neural network, with basically any other code and the model subtly melts away from being a sigular entity (think Sequential from Keras), to a part of general logic that you wish to implement. ~The adjoints are defined just as they are for any other differentiable function, again generalised from the mathematical priciples, to implementing logic consistent with what a deconstruction of the actions would look like.~\nHow the adjoints are defined #  Consider a regular function\nfunction f(arg1::CustomType, arg2, arg3...) transform1 = f1(arg1) transform2 = f2(transform1, arg3...) result = f3(transform1, transform2) result end . # going down in the abstraction . function f1(arg) result = operations_with_some_concrete_types(arg) end The function f can accept arguments with any type, including user defined ones. When we call on this function, it executes a bevy of other functions, ultimately ending with some basic operations involving concrete types (be they Arrays, Numbers, Symbols, etc), let\u0026rsquo;s call them primitives. Let\u0026rsquo;s now teach Julia how to differentiate operations involving these primitives. This would involve defining the adjoint for sqrt for a real number, for example.\n$$ \\frac{d{\\sqrt{x}}}{d{x}} = \\frac{1}{2\\sqrt{x}} $$\nWhich can be expressed as\n@adjoint Base.sqrt(x::Real) = Base.sqrt(x), Î” -\u0026gt; ( Î” * inv(2 * sqrt(x)),) The process and intuition to writing appropriate adjoints is a different blog.\nIf one could keep track of higher level operations, and define the adjoints on the primitives, we can essentially \u0026ldquo;solve back\u0026rdquo;, accumulating the resulting gradients from all the transforms (with the help of the adjoints from the primitives), and maintaining some structures, like constructing NamedTuples with the appropriate keys, we can express any operation as differentiable. The backwards pass flow would basically go something like f3 \u0026ndash;\u0026gt; f2 \u0026ndash;\u0026gt; operations_with_some_concrete_types. This way we can traverse our code (specifically, the intermediate representation), and generate the backwards passes on the fly.\nThe cool part about this approach is that if we were to define the adjoints for the primitives or the base functions of a programming language, we can get any arbitrary program to be differentiated, and even support custom types and packages, almost for free. Add in an ideal optimising compiler, and these backwards passes become efficient too!\nTo give an example, the forward pass can be thought of as the process of tying your shoelaces, and the backwards pass is when we untie them by pulling the two ends apart.\n  For a lot of this to work as expected, though, it is pertinent that the base language on top of which this entire machinery is built, exposes meaningful expressions of its intermediate representation that can be used to infer the backwards passes on the fly, and this is precisely what Julia does, given its history of hackability. Flux takes this hackability, and runs with it to the point of making sure that the entire library is focussed on inviting people to its source code and in fact extending it with their own layers and definitions and optimisers and what have you. This is a tough ask, since it means anticipating which assumptions are safe, and which aren\u0026rsquo;t, but it\u0026rsquo;s defintely worth it, since it then allows users to gracefully add in complexity as required.\nA post will be up later talking about implementing a differentiable programming solution and another explaining the guts of what makes Flux and Zygote tick.\nA Basic Optimisation Loop #  For now, let\u0026rsquo;s start with the classic example of optimising a random array to a different random array. It\u0026rsquo;s just to illustrate how a simple iterative optimisation loop is expressed in Flux.\nz = rand(3,3) zâ€² = rand(3,3) loss(x) = Flux.mse(z * x, zâ€² * x) opt = Momentum() ps = Params([z]) # z is an implicit parameter, and thus needs to be wrapped in the `Params` type. for i = 1:10^5 x = rand(3) gs = gradient(ps) do loss(x) end Flux.Optimise.update!(opt, ps, gs) end z â‰ˆ zâ€² # true And just like that, we have moved z close to zâ€²!\nAdapting this to a custom type #  Now, let\u0026rsquo;s express this in terms of our own custom struct. For simplicity\u0026rsquo;s sake, I am going to keep the fields of the struct Arrays, but they could be anything really.\nimport Base: +, -, *, / import Base: isapprox using MacroTools: @forward mutable struct GG{T} a::T b::T end GG(a) = GG(a, a) for op in (:+, :*, :-, :/) @eval @inline $(op)(a::GG, b::GG) = GG(broadcast($op, a.a, b.a), broadcast($op, a.b, b.b)) @eval @inline $(op)(a::GG, b) = GG(broadcast($op, a.a, b), broadcast($op, a.b, b)) @eval @inline $(op)(b, a::GG) = GG(broadcast($op, a.a, b), broadcast($op, a.b, b)) end @forward GG.a Base.size Here, we\u0026rsquo;ve declared the struct, and defined some basic operations on how to handle the struct and its interaction with other types. Notice how we make use of Julia\u0026rsquo;s excellent broadcasting infrastructure, and a bit of code interpolation to avoid repeating defitintions for all the operations we want to hold it to, (:+, :*, :-, :/) in this case. @inline also hints to the Julia compiler that these operations can be inlined easily, and it should try to do this optimisation if possible.\nAnd just to hit the nail on the head, let\u0026rsquo;s define some more primitives that could come in handy while optimisation. These are operations that a lot of folks would already be used to doing for mathematical compute, but we will extrapolate it to arbitrary structs, that don\u0026rsquo;t immediately make sense to be \u0026ldquo;optimisable\u0026rdquo;, in a manner of speaking.\nBase.zero(a::GG) = GG(zero(a.a), zero(a.b)) Base.length(::GG) = 1 Base.:^(a::GG, i) = GG(a.a .^ i, a.b .^ i) import Statistics: mean mean(a::GG) = mean(a.a) + mean(a.b) Base.sum(a::GG) = sum(a.a) + sum(a.b) Base.isapprox(a::GG{T}, b::GG{T}) where T = all([isapprox(a.a, b.a), isapprox(a.b, b.b)]) One last thing that might be necessary to take advantage of Flux\u0026rsquo;s optimisers is to teach it what to do with the GG struct. We can extend it to just call update on all the fields of the struct.\nfunction Flux.Optimise.update!(opt, x::T, gs, fs = fieldnames(T)) where {T\u0026lt;:GG} gs = gs.x for f in fs Flux.Optimise.update!(opt, getfield(x,f), getfield(gs,f)) end end And with that, we should be ready to do our optimisation.\nLet\u0026rsquo;s define two instances of our GG struct that we\u0026rsquo;d like to optimise.\na = GG(rand(3,3), rand(3,3)) b = GG(rand(3,3), rand(3,3)) And we will use the same Momentum optimiser and mean-squared-error loss.\nopt = Momentum() for i = 1:10^5 gs = gradient(a) do x sum((x-b) * (x-b)) / prod(size(x)) end Flux.Optimise.update!(opt, a, gs[1]) end a â‰ˆ b # true With this we have optimised a struct to another. Now we can take this concept and apply it to struct than a simple random array.\nAnother thing to note here is the complete lack of need of any call to Params in this case. This is because all of our parameters have been made explicit via passing a.\nTo give some context on the discussion earlier; the operations such as sum, prod, size, - etc are visible to Flux as valid operators to the parameters (a) and it looks into the implementation that we use for these transforms, to come up with valid adjoint methods. Think of it as the pulling motion from the shoelace example. Using these, it accumulates the gradients from all the operations, and finally returns them, keeping the structure of the paramters intact. This allows us to treat them as instances of the same type as usual, and finally optimise on them.\nOptimising Colours #  With the example done, let\u0026rsquo;s try optimising colours. This is going to get fun! This example is taken from some of our work in the differentiable programming examples that we present here.\ntarget = RGB(1, 0, 0) colour = RGB(1, 1, 1) function update_colour(c, Î”, Î· = 0.001) return RGB( c.r - Î·*Î”.r, c.g - Î·*Î”.g, c.b - Î·*Î”.b, ) end for idx in 1:51 global colour, target # Calculate gradients grads = Zygote.gradient(colour) do y colordiff(target, y) end # Update colour colour = update_colour(colour, grads[1]) if idx % 5 == 1 @info idx, colour end end Here our struct is just the RGB taken from the Colors.jl package. Again, the trick is to have meaningful operations defined on our type, based on the operations we will hit while calculating our loss function. The function colordiff already gives us the distance between two colours. It is important to note that the Descent optimiser does not check for convergence bounds and will ultimately diverge if the optimisation loop is not stopped.\nI hope this helped motivate the different aspects of making a piece of code differentiable, and how that might be useful. The implementation need not be very complicated, if we understand the basic requirements for a library like Zygote. With the coming of Optimisers.jl it should be possible to automate the optimisation over structs for many cases.\nCheers\n"},{"id":2,"href":"/docs/layer/","title":"Writing Custom Layers with Flux","section":"Docs","content":"Writing Custom Layers with Flux #  With a Unet #  Flux is a very versatile library. In particular, it doesn\u0026rsquo;t have a strict interpretation of \u0026ldquo;layers\u0026rdquo; as one would find in most libraries. In fact, in more recent research into implicit representation of models and data, we now have models with infinite layers. Instead, Flux focusses on transformations. Having said that, it is still useful to have some abstractions that keep things organised.\nThis post will focus on writing a UNet in Flux, and show how it can be used for deep learning. We will use it to write our own custom layers in the process. It is a fairly well aged network, and finds applications in medical imaging, but can be applied to a vast gamut of fields and perform image segmentation tasks in general. In that way, it is a part of natural progression from image classification to object detection and localisation to semantic segmentation and beyond. Here is a brief overview of the network in question.\nArchitecture #  UNet has been applied to a large number of problems like Autonomous Driving, since it is useful to detect lane markings, traffic signs and even free space for the cars to move into. It has also been applied to Geo-Sensing and Land Cover analysis for helping with various projects including city planning and traffic management. You might be interested in this blog that goes deeper into UNet itself.\nNo points for guessing how it got its name, though.\nFurther, since its a fully convolutional model, it can also be used by images of arbitrary sizes.\nThe reference implementation of the model in PyTorch can be found here.\nWriting Layers #  Writing a layer in Flux is actually pretty straightforward, getting rid of most boilerplate code. Ii is fairly standard practice to define ones own structs to use in specialisation and method dispatch in Julia. We will use structs to define our layers.\nThinking of the components to define a layer, we need to figure out what kinds of parameters it can hold, and what happens when the layer is fed some input. For this post, we can assume the inputs are going to be regular arrays.\nThe general style guide to define the layer, would look so:\nstruct MyLayer a b c end function (a::MyLayer)(x::AbstractArray) # do something... end Here, our layer would hold some parameters (and peripheral details; think padding for a convolutional layer) in its fields (a,b,c...). We then make this layer callable (known as a functor); this isn\u0026rsquo;t strictly necessary since we can define a normal function that takes a layer object explicitly, like usual, but doing this allows us to use layer in a much more natural looking manner:\nMyLayer(input) There is one additional operation that Flux expects, which is calling @functor MyLayer. It makes it such that all the parameters of our layer are visible to the AD, while backpropagating. This can be thought of in a way \u0026ldquo;registering\u0026rdquo; the layer to take advantage of the rest of the machinery.\nNote: that if only certain fields are designed to be treated as parameters, leaving the rest of them untouched, it is possible to call  @functor MyLayer a, b Beyond this, we will exploit one other possibility that this opens up. This is the ability to compose layers together, creating higher-order layers. Composing these layers together basically makes up the models, with their own forward pass implicitly defined.\nLet me demonstrate that with an example. The UNet has a bunch of structures within itself that is symmetric around the smaller convolutional structure. We\u0026rsquo;ll call them UNetUpBlock, as it does upsampling.\nstruct UNetUpBlock upsample end @functor UNetUpBlock function (u::UNetUpBlock)(x, bridge) x = u.upsample(x) return cat(x, bridge, dims = 3) end So far so good. This looks very similar to the template we had prepared from earlier. Now, let\u0026rsquo;s add some convenience methods to UNetUpBlock so its simpler to construct.\nUNetUpBlock(in_chs::Int, out_chs::Int; kernel = (3, 3), p = 0.5f0) = UNetUpBlock(Chain(x -\u0026gt; leakyrelu.(x,0.2f0), ConvTranspose((2, 2), in_chs =\u0026gt; out_chs, stride = (2, 2); init = _random_normal), BatchNormWrap(out_chs)..., Dropout(p))) I\u0026rsquo;ve left the definition of some functions (such as BatchNormWrap and _random_normal) out of this tutorial for clarity, but all the functions can be found in UNet.jl.  Notice, it isn\u0026rsquo;t strictly necessary to write a new layer in this case, since we really just need the forward pass to run on the upsample field, which in itself is just a tiny model! Here, defining this layer only really is helping us avoid rewriting the forward pass with the concatenation multiple times. If we hadn\u0026rsquo;t needed this, it is possible to just define a simple function that captures the cat function with the x and bridge variables. Next, we need to downsample. This would look like a mirror of the UNetUpBlock, the ConvDown layer:\nConvDown(in_chs,out_chs,kernel = (4,4)) = Chain(Conv(kernel,in_chs=\u0026gt;out_chs, pad=(1,1), stride=(2,2); init=_random_normal), BatchNormWrap(out_chs)..., x -\u0026gt; leakyrelu.(x, 0.2f0)) We can now use these layers to simplify our construction of the actual UNet, which can itself be described as just another layer.\nstruct UNet conv_down_blocks conv_blocks up_blocks end @functor UNet function UNet() conv_down_blocks = Chain(ConvDown(64,64), ConvDown(128,128), ConvDown(256,256), ConvDown(512,512)) conv_blocks = Chain(UNetConvBlock(1, 3), UNetConvBlock(3, 64), UNetConvBlock(64, 128), UNetConvBlock(128, 256), UNetConvBlock(256, 512), UNetConvBlock(512, 1024), UNetConvBlock(1024, 1024)) up_blocks = Chain(UNetUpBlock(1024, 512), UNetUpBlock(1024, 256), UNetUpBlock(512, 128), UNetUpBlock(256, 64,p = 0.0f0), Chain(x-\u0026gt;leakyrelu.(x,0.2f0), Conv((1, 1), 128=\u0026gt;1;init=_random_normal))) UNet(conv_down_blocks, conv_blocks, up_blocks) end The actual definition of the model is quite a bit clearer and makes it very obvious how the model is layed out. The left side of the model can be mapped to the conv_down_blocks and the upsampling ones to the up_blocks. Neat.\nNow to define the forward pass, we just need to remember that we want to first downsample the incoming image, apply the conv blocks, and finally upsample it back up to the size of the image. Remember that we modeled the whole thing as a layer? Well we can just define what we spoke about here as the forward pass. Following the paper, it would look a little like so:\nfunction (u::UNet)(x) outputs = Vector(undef, 5) outputs[1] = u.conv_blocks[1:2](x) for i in 2:5 pool_x = u.conv_down_blocks[i - 1](outputs[i - 1]) outputs[i] = u.conv_blocks[i+1](pool_x) end up_x = u.conv_blocks[7](outputs[end]) for i in 1:4 up_x = u.up_blocks[i](up_x, outputs[end - i]) end tanh.(u.up_blocks[end](up_x)) end Since Flux can differentiate arbitrary code, we can take some liberties to define the layers and their forward passes like we did, eliminating any boilerplate, or specialised \u0026ldquo;blessed\u0026rdquo; functions to allow us to express the problem naturally. This way of defining layers is just one way to express them. As eluded to before, any function/ operation/ transformation is considered a layer in itself, with most having their gradients calculated on the fly.\nThe expressiveness the framework allows is one of those subtle differences from how most libraries are laid out. It also means we can just as easily start using other packages in our models as well. No need to have specially designed \u0026ldquo;differentiable\u0026rdquo; versions of tools, since in Julia, everything is differenitable by default. Well, almost everything ðŸ˜‰\nCheers\n"}]