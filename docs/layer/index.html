<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Writing Custom Layers with Flux - With a Unet # Flux is a very versatile library. In particular, it doesn&rsquo;t have a strict interpretation of &ldquo;layers&rdquo; as one would find in most libraries. In fact, in more recent research into implicit representation of models and data, we now have models with infinite layers. Instead, Flux focusses on transformations. Having said that, it is still useful to have some abstractions that keep things organised."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="Writing Custom Layers with Flux"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://dhairyalgandhi.github.io/docs/layer/"><title>Writing Custom Layers with Flux | </title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.95d69eb6bad8b9707ff2b5d8d9e31ce70a1b84f2ed7ffaf665ffcf00aa7993bd.css integrity="sha256-ldaetrrYuXB/8rXY2eMc5wobhPLtf/r2Zf/PAKp5k70=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.9759bb8c95081e0434a27b43e48131f7b129860cc77920f48ae25b2fa90d3d91.js integrity="sha256-l1m7jJUIHgQ0ontD5IEx97EphgzHeSD0iuJbL6kNPZE=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=https://dhairyalgandhi.github.io/docs/layer/index.xml title></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=https://dhairyalgandhi.github.io/docs/blog/>Differentiable Programming is Easy with Flux</a><ul></ul></li><li><a href=https://dhairyalgandhi.github.io/docs/layer/ class=active>Writing Custom Layers with Flux</a><ul></ul></li></ul><a href=https://wandb.ai/dhairyalgandhi/DDP_Trantor/reports/Data-Parallel-Training-with-Flux-jl--VmlldzoxNTA0Mjk5>Distributed Data Parallel Training in Julia</a><br><a href=mailto:dhairyagandhi96@gmail.com><img src=https://img.icons8.com/material-sharp/30/000000/mail.png title="dhairyagandhi96 [AT] gmail.com"></a>
<a href=https://github.com/DhairyaLGandhi><img src=https://img.icons8.com/material-rounded/30/000000/github.png></a>
<a href="https://scholar.google.com/citations?hl=en&user=9i0Z2xkAAAAJ"><img src=https://img.icons8.com/ios-glyphs/30/000000/graduation-cap--v1.png></a></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Writing Custom Layers with Flux</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#architecture>Architecture</a></li><li><a href=#writing-layers>Writing Layers</a><ul><li><a href=#basic-scaffolding-for-a-layer>Basic Scaffolding for a Layer</a></li><li><a href=#simple-explanation>Simple Explanation</a></li><li><a href=#translating-this-pattern-to-a-unet>Translating this Pattern to a UNet</a></li></ul></li><li><a href=#composing-layers-together>Composing Layers Together</a></li><li><a href=#putting-all-the-pieces-together>Putting All the Pieces Together</a></li><li><a href=#closing-thoughts>Closing Thoughts</a></li></ul></nav></aside></header><article class=markdown><h1 id=writing-custom-layers-with-flux---with-a-unet>Writing Custom Layers with Flux - With a Unet
<a class=anchor href=#writing-custom-layers-with-flux---with-a-unet>#</a></h1><p>Flux is a very versatile library. In particular, it doesn&rsquo;t have a strict interpretation of &ldquo;layers&rdquo; as one would find in most libraries. In fact, in more recent research into implicit representation of models and data, we now have models with <em>infinite</em> layers. Instead, Flux focusses on transformations. Having said that, it is still useful to have some abstractions that keep things organised.</p><p>This post will focus on writing a UNet in Flux, and show how it can be used for deep learning. We will use it to write our own custom layers in the process. It is a fairly well aged network, and finds applications in medical imaging, but can be applied to a vast gamut of fields and perform image segmentation tasks in general. In that way, it is a part of natural progression from image classification to object detection and localisation to semantic segmentation and beyond. Here is a brief overview of the network in question.</p><h2 id=architecture>Architecture
<a class=anchor href=#architecture>#</a></h2><p><img src=https://miro.medium.com/max/3110/1*lvXoKMHoPJMKpKK7keZMEA.png alt=unet-image></p><p>UNet has been applied to a large number of problems like Autonomous Driving, since it is useful to detect lane markings, traffic signs and even free space for the cars to move into. It has also been applied to Geo-Sensing and Land Cover analysis for helping with various projects including city planning and traffic management. You might be interested in <a href=https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47>this</a> blog that goes deeper into UNet itself.</p><p>No points for guessing how it got its name, though.</p><p>Further, since its a fully convolutional model, it can also be used by images of arbitrary sizes.</p><p>The reference implementation of the model in PyTorch can be found <a href=https://github.com/milesial/Pytorch-UNet>here</a>.</p><h2 id=writing-layers>Writing Layers
<a class=anchor href=#writing-layers>#</a></h2><p>Writing a layer in Flux is actually pretty straightforward, getting rid of most boilerplate code. Ii is fairly standard practice to define ones own <code>struct</code>s to use in specialisation and method dispatch in Julia. We will use <code>struct</code>s to define our layers.</p><p>Thinking of the components to define a layer, we need to figure out what kinds of parameters it can hold, and what happens when the layer is fed some input. For this post, we can assume the inputs are going to be regular arrays.</p><h3 id=basic-scaffolding-for-a-layer>Basic Scaffolding for a Layer
<a class=anchor href=#basic-scaffolding-for-a-layer>#</a></h3><p>The general style guide to define the layer, would look so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>MyLayer</span>
</span></span><span style=display:flex><span>  a
</span></span><span style=display:flex><span>  b
</span></span><span style=display:flex><span>  c
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> (a<span style=color:#f92672>::</span><span style=color:#66d9ef>MyLayer</span>)(x<span style=color:#f92672>::</span><span style=color:#66d9ef>AbstractArray</span>)
</span></span><span style=display:flex><span>  <span style=color:#75715e># do something...</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><h3 id=simple-explanation>Simple Explanation
<a class=anchor href=#simple-explanation>#</a></h3><p>Here, our layer would hold some parameters (and peripheral details; think padding for a convolutional layer) in its fields (<code>a,b,c...</code>). We then make this layer <em>callable</em> (known as a <code>functor</code>); this isn&rsquo;t strictly necessary since we can define a normal function that takes a layer object explicitly, like usual, but doing this allows us to use layer in a much more natural looking manner:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>MyLayer(input)
</span></span></code></pre></div><p>There is one additional operation that Flux expects, which is calling <code>@functor MyLayer</code>. It makes it such that all the parameters of our layer are visible to the AD, while backpropagating. This can be thought of in a way &ldquo;registering&rdquo; the layer to take advantage of the rest of the machinery.</p><blockquote class="book-hint info"><p>Note: that if only certain fields are designed to be treated as parameters, leaving the rest of them untouched, it is possible to call</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#a6e22e>@functor</span> MyLayer a, b
</span></span></code></pre></div></blockquote><p>Beyond this, we will exploit one other possibility that this opens up. This is the ability to compose layers together, creating higher-order layers. Composing these layers together basically makes up the models, with their own forward pass implicitly defined.</p><h3 id=translating-this-pattern-to-a-unet>Translating this Pattern to a UNet
<a class=anchor href=#translating-this-pattern-to-a-unet>#</a></h3><p>Let me demonstrate that with an example. The UNet has a bunch of structures within itself that is symmetric around the smaller convolutional structure. We&rsquo;ll call them <code>UNetUpBlock</code>, as it does upsampling.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>UNetUpBlock</span>
</span></span><span style=display:flex><span>  upsample
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@functor</span> UNetUpBlock
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> (u<span style=color:#f92672>::</span><span style=color:#66d9ef>UNetUpBlock</span>)(x, bridge)
</span></span><span style=display:flex><span>  x <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>upsample(x)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> cat(x, bridge, dims <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>So far so good. This looks very similar to the template we had prepared from earlier. Now, let&rsquo;s add some convenience methods to <code>UNetUpBlock</code> so its simpler to construct.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>UNetUpBlock(in_chs<span style=color:#f92672>::</span><span style=color:#66d9ef>Int</span>, out_chs<span style=color:#f92672>::</span><span style=color:#66d9ef>Int</span>; kernel <span style=color:#f92672>=</span> (<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), p <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5f0</span>) <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    UNetUpBlock(Chain(x <span style=color:#f92672>-&gt;</span> leakyrelu<span style=color:#f92672>.</span>(x,<span style=color:#ae81ff>0.2f0</span>),
</span></span><span style=display:flex><span>                ConvTranspose((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), in_chs <span style=color:#f92672>=&gt;</span> out_chs,
</span></span><span style=display:flex><span>                        stride <span style=color:#f92672>=</span> (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>); init <span style=color:#f92672>=</span> _random_normal),
</span></span><span style=display:flex><span>                BatchNormWrap(out_chs)<span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>                Dropout(p)))
</span></span></code></pre></div><blockquote class="book-hint warning">I&rsquo;ve left the definition of some functions (such as <code>BatchNormWrap</code> and <code>_random_normal</code>) out of this tutorial for clarity, but all the functions can be found in <a href=https://github.com/DhairyaLGandhi/UNet.jl><code>UNet.jl</code></a>.</blockquote><p>Notice, it isn&rsquo;t strictly necessary to write a new layer in this case, since we really just need the forward pass to run on the <code>upsample</code> field, which in itself is just a tiny model! Here, defining this layer only really is helping us avoid rewriting the forward pass with the concatenation multiple times. If we hadn&rsquo;t needed this, it is possible to just define a simple function that captures the <code>cat</code> function with the <code>x</code> and <code>bridge</code> variables. Next, we need to downsample. This would look like a mirror of the <code>UNetUpBlock</code>, the <code>ConvDown</code> layer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span>ConvDown(in_chs,out_chs,kernel <span style=color:#f92672>=</span> (<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>4</span>)) <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	Chain(Conv(kernel,in_chs<span style=color:#f92672>=&gt;</span>out_chs,
</span></span><span style=display:flex><span>		   pad<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>		   stride<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>		   init<span style=color:#f92672>=</span>_random_normal),
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	      BatchNormWrap(out_chs)<span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>	      x <span style=color:#f92672>-&gt;</span> leakyrelu<span style=color:#f92672>.</span>(x, <span style=color:#ae81ff>0.2f0</span>))
</span></span></code></pre></div><h2 id=composing-layers-together>Composing Layers Together
<a class=anchor href=#composing-layers-together>#</a></h2><p>We can now use these layers to simplify our construction of the actual UNet, which can itself be described as just another layer.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>UNet</span>
</span></span><span style=display:flex><span>  conv_down_blocks
</span></span><span style=display:flex><span>  conv_blocks
</span></span><span style=display:flex><span>  up_blocks
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@functor</span> UNet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> UNet()
</span></span><span style=display:flex><span>  conv_down_blocks <span style=color:#f92672>=</span> Chain(ConvDown(<span style=color:#ae81ff>64</span>,<span style=color:#ae81ff>64</span>),
</span></span><span style=display:flex><span>                      ConvDown(<span style=color:#ae81ff>128</span>,<span style=color:#ae81ff>128</span>),
</span></span><span style=display:flex><span>                      ConvDown(<span style=color:#ae81ff>256</span>,<span style=color:#ae81ff>256</span>),
</span></span><span style=display:flex><span>                      ConvDown(<span style=color:#ae81ff>512</span>,<span style=color:#ae81ff>512</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  conv_blocks <span style=color:#f92672>=</span> Chain(UNetConvBlock(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>),
</span></span><span style=display:flex><span>                 UNetConvBlock(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>64</span>),
</span></span><span style=display:flex><span>                 UNetConvBlock(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>),
</span></span><span style=display:flex><span>                 UNetConvBlock(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>),
</span></span><span style=display:flex><span>                 UNetConvBlock(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>512</span>),
</span></span><span style=display:flex><span>                 UNetConvBlock(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>1024</span>),
</span></span><span style=display:flex><span>                 UNetConvBlock(<span style=color:#ae81ff>1024</span>, <span style=color:#ae81ff>1024</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  up_blocks <span style=color:#f92672>=</span> Chain(UNetUpBlock(<span style=color:#ae81ff>1024</span>, <span style=color:#ae81ff>512</span>),
</span></span><span style=display:flex><span>                UNetUpBlock(<span style=color:#ae81ff>1024</span>, <span style=color:#ae81ff>256</span>),
</span></span><span style=display:flex><span>                UNetUpBlock(<span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>128</span>),
</span></span><span style=display:flex><span>                UNetUpBlock(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>64</span>,p <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0f0</span>),
</span></span><span style=display:flex><span>                Chain(x<span style=color:#f92672>-&gt;</span>leakyrelu<span style=color:#f92672>.</span>(x,<span style=color:#ae81ff>0.2f0</span>),
</span></span><span style=display:flex><span>                Conv((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), <span style=color:#ae81ff>128</span><span style=color:#f92672>=&gt;</span><span style=color:#ae81ff>1</span>;init<span style=color:#f92672>=</span>_random_normal)))
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  UNet(conv_down_blocks, conv_blocks, up_blocks)
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>The actual definition of the model is quite a bit clearer and makes it very obvious how the model is layed out. The left side of the model can be mapped to the <code>conv_down_blocks</code> and the upsampling ones to the <code>up_blocks</code>. Neat.</p><h2 id=putting-all-the-pieces-together>Putting All the Pieces Together
<a class=anchor href=#putting-all-the-pieces-together>#</a></h2><p>Now to define the forward pass, we just need to remember that we want to first downsample the incoming image, apply the conv blocks, and finally upsample it back up to the size of the image. Remember that we modeled the whole thing as a layer? Well we can just define what we spoke about here as the forward pass. Following the <a href=https://arxiv.org/pdf/1505.04597.pdf>paper</a>, it would look a little like so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-julia data-lang=julia><span style=display:flex><span><span style=color:#66d9ef>function</span> (u<span style=color:#f92672>::</span><span style=color:#66d9ef>UNet</span>)(x)
</span></span><span style=display:flex><span>  outputs <span style=color:#f92672>=</span> <span style=color:#66d9ef>Vector</span>(undef, <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>  outputs[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>conv_blocks[<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>2</span>](x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> i <span style=color:#66d9ef>in</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span><span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>    pool_x <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>conv_down_blocks[i <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>](outputs[i <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    outputs[i] <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>conv_blocks[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>](pool_x)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  up_x <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>conv_blocks[<span style=color:#ae81ff>7</span>](outputs[<span style=color:#66d9ef>end</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> i <span style=color:#66d9ef>in</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>    up_x <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>up_blocks[i](up_x, outputs[<span style=color:#66d9ef>end</span> <span style=color:#f92672>-</span> i])
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  tanh<span style=color:#f92672>.</span>(u<span style=color:#f92672>.</span>up_blocks[<span style=color:#66d9ef>end</span>](up_x))
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span></code></pre></div><p>Since Flux can differentiate arbitrary code, we can take some liberties to define the layers and their forward passes like we did, eliminating any boilerplate, or specialised &ldquo;blessed&rdquo; functions to allow us to express the problem naturally. This way of defining layers is just one way to express them. As eluded to before, any function/ operation/ transformation is considered a layer in itself, with most having their gradients calculated on the fly.</p><h2 id=closing-thoughts>Closing Thoughts
<a class=anchor href=#closing-thoughts>#</a></h2><p>The expressiveness the framework allows is one of those subtle differences from how most libraries are laid out. It also means we can just as easily start using other packages in our models as well. No need to have specially designed &ldquo;differentiable&rdquo; versions of tools, since in Julia, everything is differenitable by default. Well, almost everything &#x1f609;</p><p><em>Cheers</em></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#architecture>Architecture</a></li><li><a href=#writing-layers>Writing Layers</a><ul><li><a href=#basic-scaffolding-for-a-layer>Basic Scaffolding for a Layer</a></li><li><a href=#simple-explanation>Simple Explanation</a></li><li><a href=#translating-this-pattern-to-a-unet>Translating this Pattern to a UNet</a></li></ul></li><li><a href=#composing-layers-together>Composing Layers Together</a></li><li><a href=#putting-all-the-pieces-together>Putting All the Pieces Together</a></li><li><a href=#closing-thoughts>Closing Thoughts</a></li></ul></nav></div></aside></main></body></html>